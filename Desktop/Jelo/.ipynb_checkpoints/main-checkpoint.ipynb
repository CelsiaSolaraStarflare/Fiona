{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5501b2-c5cc-4c95-8a1b-40b7adad3a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exports DataFrame:\n",
      "      State        Commodity       Country    Time Vessel Value ($US)  \\\n",
      "0  Illinois  01 Live Animals        Africa  Jul-16              7,185   \n",
      "2  Illinois  01 Live Animals  Asia - South  Jun-18             15,087   \n",
      "5  Illinois  01 Live Animals  Asia - Other  Sep-09              5,560   \n",
      "6  Illinois  01 Live Animals  Asia - Other  Dec-11            123,178   \n",
      "7  Illinois  01 Live Animals  Asia - Other  Jan-12             85,860   \n",
      "\n",
      "  Containerized Vessel Total Exports Value ($US) Vessel SWT (kg)  \\\n",
      "0                                          7,185               6   \n",
      "2                                         15,087           1,422   \n",
      "5                                          5,560           1,236   \n",
      "6                                        123,178           1,142   \n",
      "7                                         85,860           1,681   \n",
      "\n",
      "  Containerized Vessel Total Exports SWT (kg)  \n",
      "0                                           6  \n",
      "2                                       1,422  \n",
      "5                                       1,236  \n",
      "6                                       1,142  \n",
      "7                                       1,681  \n",
      "\n",
      "Imports DataFrame:\n",
      "      State        Commodity       Country    Time Vessel Value ($US)  \\\n",
      "0  Illinois  01 Live Animals  Asia - Other  Aug-19             11,840   \n",
      "1  Illinois  01 Live Animals  Asia - Other  Oct-19              9,472   \n",
      "2  Illinois  01 Live Animals  Asia - Other  Nov-19              4,440   \n",
      "3  Illinois  01 Live Animals  Asia - Other  Dec-19             10,360   \n",
      "4  Illinois  01 Live Animals  Asia - Other  Feb-20             11,840   \n",
      "\n",
      "  Customs Containerized Vessel Value (Gen) ($US) Vessel SWT (kg)  \\\n",
      "0                                         11,840          10,200   \n",
      "1                                          9,472           8,200   \n",
      "2                                          4,440           3,960   \n",
      "3                                         10,360           8,800   \n",
      "4                                         11,840          10,100   \n",
      "\n",
      "  Containerized Vessel SWT (Gen) (kg)  \n",
      "0                              10,200  \n",
      "1                               8,200  \n",
      "2                               3,960  \n",
      "3                               8,800  \n",
      "4                              10,100  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files, skipping the first row (metadata) and using the second row as the header\n",
    "dfe = pd.read_csv(\"custom_data/export/Illinois.csv\", skiprows=1, header=1)\n",
    "dfi = pd.read_csv(\"custom_data/import/Illinois.csv\", skiprows=1, header=1)\n",
    "\n",
    "#Drop the missing things\n",
    "dfe = dfe.dropna()\n",
    "dfi = dfi.dropna()\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(\"Exports DataFrame:\")\n",
    "print(dfe.head())\n",
    "print(\"\\nImports DataFrame:\")\n",
    "print(dfi.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba3dff05-d51b-483a-a448-ddab0193ee55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['State', 'Commodity', 'Country', 'Time', 'Vessel Value ($US)',\n",
      "       'Containerized Vessel Total Exports Value ($US)', 'Vessel SWT (kg)',\n",
      "       'Containerized Vessel Total Exports SWT (kg)'],\n",
      "      dtype='object')\n",
      "       Containerized Vessel Total Exports Value ($US)  \\\n",
      "0                                               7,185   \n",
      "2                                              15,087   \n",
      "5                                               5,560   \n",
      "6                                             123,178   \n",
      "7                                              85,860   \n",
      "...                                               ...   \n",
      "110705                                          5,457   \n",
      "110706                                         20,219   \n",
      "110707                                          5,004   \n",
      "110708                                         28,620   \n",
      "110709                                         10,898   \n",
      "\n",
      "       Containerized Vessel Total Exports SWT (kg)  \n",
      "0                                                6  \n",
      "2                                            1,422  \n",
      "5                                            1,236  \n",
      "6                                            1,142  \n",
      "7                                            1,681  \n",
      "...                                            ...  \n",
      "110705                                         361  \n",
      "110706                                      10,055  \n",
      "110707                                       1,547  \n",
      "110708                                       6,486  \n",
      "110709                                       2,239  \n",
      "\n",
      "[104170 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dfe.columns)\n",
    "dfe_value = dfe[['Containerized Vessel Total Exports Value ($US)','Containerized Vessel Total Exports SWT (kg)']]\n",
    "print(dfe_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c985b2-c033-4d2f-9b1f-4eeee9f34586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Time                                    Commodity     State  \\\n",
      "0       Jul-16                              01 Live Animals  Illinois   \n",
      "2       Jun-18                              01 Live Animals  Illinois   \n",
      "5       Sep-09                              01 Live Animals  Illinois   \n",
      "6       Dec-11                              01 Live Animals  Illinois   \n",
      "7       Jan-12                              01 Live Animals  Illinois   \n",
      "...        ...                                          ...       ...   \n",
      "110705  Jul-24  98 Special Classification Provisions, Nesoi  Illinois   \n",
      "110706  Dec-24  98 Special Classification Provisions, Nesoi  Illinois   \n",
      "110707  Feb-25  98 Special Classification Provisions, Nesoi  Illinois   \n",
      "110708  Mar-25  98 Special Classification Provisions, Nesoi  Illinois   \n",
      "110709  Apr-25  98 Special Classification Provisions, Nesoi  Illinois   \n",
      "\n",
      "              Country  \n",
      "0              Africa  \n",
      "2        Asia - South  \n",
      "5        Asia - Other  \n",
      "6        Asia - Other  \n",
      "7        Asia - Other  \n",
      "...               ...  \n",
      "110705  South America  \n",
      "110706  South America  \n",
      "110707  South America  \n",
      "110708  South America  \n",
      "110709  South America  \n",
      "\n",
      "[104170 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dfe = dfe[['Time','Commodity','State','Country']]\n",
    "print(dfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2783776f-d940-4afb-b638-3c96b6e0f77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1197.5          10.60970464    4.49838188 ...    3.23464771    4.41258094\n",
      "    4.8673515 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "value = dfe_value.iloc[:, 0].str.replace(',', '').astype(float).to_numpy()\n",
    "weight = dfe_value.iloc[:, 1].str.replace(',', '').astype(float).to_numpy()\n",
    "\n",
    "dfe_per_kg_value = value / weight\n",
    "print(dfe_per_kg_value)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7f251c0-0f55-4476-90ed-4532eb50da57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([104170, 4])\n",
      "tensor([[  0,  96,   0,   0],\n",
      "        [  0, 115,   0,   2],\n",
      "        [  0, 193,   0,   1],\n",
      "        ...,\n",
      "        [  0,  69,  96,   8],\n",
      "        [  0, 139,  96,   8],\n",
      "        [  0,  17,  96,   8]])\n",
      "\n",
      "Exports DataFrame:\n",
      "     Time        Commodity     State       Country\n",
      "0  Jul-16  01 Live Animals  Illinois        Africa\n",
      "2  Jun-18  01 Live Animals  Illinois  Asia - South\n",
      "5  Sep-09  01 Live Animals  Illinois  Asia - Other\n",
      "6  Dec-11  01 Live Animals  Illinois  Asia - Other\n",
      "7  Jan-12  01 Live Animals  Illinois  Asia - Other\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Convert columns to categorical and get integer codes\n",
    "state_ids = dfe['State'].astype('category').cat.codes\n",
    "commodity_ids = dfe['Commodity'].astype('category').cat.codes\n",
    "country_ids = dfe['Country'].astype('category').cat.codes\n",
    "time_ids = dfe['Time'].astype('category').cat.codes  # Treat Time as categorical\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "state_tensor = torch.tensor(state_ids.values, dtype=torch.long)\n",
    "time_tensor = torch.tensor(time_ids.values, dtype=torch.long)\n",
    "commodity_tensor = torch.tensor(commodity_ids.values, dtype=torch.long)\n",
    "country_tensor = torch.tensor(country_ids.values, dtype=torch.long)\n",
    "\n",
    "# Combine tensors\n",
    "combined_input = torch.stack([state_tensor, time_tensor, commodity_tensor, country_tensor], dim=1)\n",
    "\n",
    "# Print shape and tensor\n",
    "print(combined_input.shape)\n",
    "print(combined_input)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "print(\"\\nExports DataFrame:\")\n",
    "print(dfe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e0405b5-8159-4a3d-9748-acfbc9c63592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d5306-6353-47c8-8f32-cf95db6f0fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef9e38b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "140eb627",
   "metadata": {},
   "source": [
    "# Training Setup and Execution\n",
    "\n",
    "This section sets up and executes the S4 model training for commodity export prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df669ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training modules\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../Predictor')  # Add the Predictor directory to path\n",
    "\n",
    "# Import our custom training modules\n",
    "try:\n",
    "    from train import CommodityExportDataset, CommodityS4Model\n",
    "    print(\"‚úì Successfully imported training modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚úó Error importing training modules: {e}\")\n",
    "    print(\"Make sure the Predictor directory contains the modified train.py\")\n",
    "\n",
    "# Set up training parameters\n",
    "TRAINING_CONFIG = {\n",
    "    'data_path': 'export/Illinois.csv',\n",
    "    'sequence_length': 12,\n",
    "    'epochs': 20,\n",
    "    'd_model': 128,\n",
    "    'n_layers': 4,\n",
    "    'dropout': 0.2,\n",
    "    'learning_rate': 0.01,\n",
    "    'batch_size': 32,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "\n",
    "print(\"‚úì Training configuration set up\")\n",
    "print(f\"Configuration: {TRAINING_CONFIG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4049b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and validate dataset\n",
    "print(\"üîÑ Creating commodity export dataset...\")\n",
    "\n",
    "try:\n",
    "    # Create training and test datasets\n",
    "    train_dataset = CommodityExportDataset(\n",
    "        data_path=TRAINING_CONFIG['data_path'],\n",
    "        sequence_length=TRAINING_CONFIG['sequence_length'],\n",
    "        train=True\n",
    "    )\n",
    "    \n",
    "    test_dataset = CommodityExportDataset(\n",
    "        data_path=TRAINING_CONFIG['data_path'],\n",
    "        sequence_length=TRAINING_CONFIG['sequence_length'],\n",
    "        train=False\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Training dataset created: {len(train_dataset)} samples\")\n",
    "    print(f\"‚úì Test dataset created: {len(test_dataset)} samples\")\n",
    "    \n",
    "    # Get dataset information\n",
    "    sample_seq, sample_commodity, sample_country, sample_value, sample_weight = train_dataset[0]\n",
    "    d_input = sample_seq.shape[1]\n",
    "    n_commodities = len(train_dataset.commodity_encoder.classes_)\n",
    "    n_countries = len(train_dataset.country_encoder.classes_)\n",
    "    \n",
    "    print(f\"‚úì Input features: {d_input}\")\n",
    "    print(f\"‚úì Sequence length: {TRAINING_CONFIG['sequence_length']}\")\n",
    "    print(f\"‚úì Number of commodities: {n_commodities}\")\n",
    "    print(f\"‚úì Number of countries: {n_countries}\")\n",
    "    \n",
    "    # Store dataset info for model creation\n",
    "    DATASET_INFO = {\n",
    "        'd_input': d_input,\n",
    "        'n_commodities': n_commodities,\n",
    "        'n_countries': n_countries,\n",
    "        'train_size': len(train_dataset),\n",
    "        'test_size': len(test_dataset)\n",
    "    }\n",
    "    \n",
    "    print(\"‚úì Dataset validation completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error creating dataset: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f9503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and training components\n",
    "print(\"üîÑ Setting up model and training components...\")\n",
    "\n",
    "try:\n",
    "    # Create the model\n",
    "    model = CommodityS4Model(\n",
    "        d_input=DATASET_INFO['d_input'],\n",
    "        n_commodities=DATASET_INFO['n_commodities'],\n",
    "        n_countries=DATASET_INFO['n_countries'],\n",
    "        d_model=TRAINING_CONFIG['d_model'],\n",
    "        n_layers=TRAINING_CONFIG['n_layers'],\n",
    "        dropout=TRAINING_CONFIG['dropout'],\n",
    "        prenorm=False\n",
    "    )\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    print(f\"‚úì Model created and moved to {device}\")\n",
    "    print(f\"‚úì Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    from torch.utils.data import DataLoader, random_split\n",
    "    \n",
    "    # Split training data into train/validation\n",
    "    train_size = int(0.9 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_subset, val_subset = random_split(\n",
    "        train_dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_subset, \n",
    "        batch_size=TRAINING_CONFIG['batch_size'], \n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_subset, \n",
    "        batch_size=TRAINING_CONFIG['batch_size'], \n",
    "        shuffle=False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=TRAINING_CONFIG['batch_size'], \n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Data loaders created:\")\n",
    "    print(f\"  - Training batches: {len(train_loader)}\")\n",
    "    print(f\"  - Validation batches: {len(val_loader)}\")\n",
    "    print(f\"  - Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    # Create loss functions and optimizer\n",
    "    criterion_value = torch.nn.MSELoss()\n",
    "    criterion_weight = torch.nn.MSELoss()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=TRAINING_CONFIG['learning_rate'],\n",
    "        weight_decay=TRAINING_CONFIG['weight_decay']\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, \n",
    "        T_max=TRAINING_CONFIG['epochs']\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Loss functions, optimizer, and scheduler created\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error setting up training components: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ff872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "def train_epoch(model, train_loader, criterion_value, criterion_weight, optimizer, device):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_value_loss = 0\n",
    "    total_weight_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (sequences, commodities, countries, value_targets, weight_targets) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        sequences = sequences.to(device)\n",
    "        commodities = commodities.to(device)\n",
    "        countries = countries.to(device)\n",
    "        value_targets = value_targets.to(device)\n",
    "        weight_targets = weight_targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        value_outputs, weight_outputs = model(sequences, commodities, countries)\n",
    "        \n",
    "        # Calculate losses\n",
    "        loss_value = criterion_value(value_outputs, value_targets)\n",
    "        loss_weight = criterion_weight(weight_outputs, weight_targets)\n",
    "        total_loss_batch = loss_value + loss_weight\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss_batch.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate losses\n",
    "        total_loss += total_loss_batch.item()\n",
    "        total_value_loss += loss_value.item()\n",
    "        total_weight_loss += loss_weight.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Print progress every 10 batches\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"  Batch {batch_idx}/{len(train_loader)} - \"\n",
    "                  f\"Total: {total_loss_batch.item():.4f}, \"\n",
    "                  f\"Value: {loss_value.item():.4f}, \"\n",
    "                  f\"Weight: {loss_weight.item():.4f}\")\n",
    "    \n",
    "    return total_loss / num_batches, total_value_loss / num_batches, total_weight_loss / num_batches\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion_value, criterion_weight, device):\n",
    "    \"\"\"Evaluate the model on a dataset\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_value_loss = 0\n",
    "    total_weight_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, commodities, countries, value_targets, weight_targets in data_loader:\n",
    "            # Move data to device\n",
    "            sequences = sequences.to(device)\n",
    "            commodities = commodities.to(device)\n",
    "            countries = countries.to(device)\n",
    "            value_targets = value_targets.to(device)\n",
    "            weight_targets = weight_targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            value_outputs, weight_outputs = model(sequences, commodities, countries)\n",
    "            \n",
    "            # Calculate losses\n",
    "            loss_value = criterion_value(value_outputs, value_targets)\n",
    "            loss_weight = criterion_weight(weight_outputs, weight_targets)\n",
    "            total_loss_batch = loss_value + loss_weight\n",
    "            \n",
    "            # Accumulate losses\n",
    "            total_loss += total_loss_batch.item()\n",
    "            total_value_loss += loss_value.item()\n",
    "            total_weight_loss += loss_weight.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches, total_value_loss / num_batches, total_weight_loss / num_batches\n",
    "\n",
    "print(\"‚úì Training and evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop - THE BOOTER!\n",
    "print(\"üöÄ Starting commodity export prediction training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training history for plotting\n",
    "training_history = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'train_value_loss': [],\n",
    "    'train_weight_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_value_loss': [],\n",
    "    'val_weight_loss': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(TRAINING_CONFIG['epochs']):\n",
    "        print(f\"\\nüìä Epoch {epoch + 1}/{TRAINING_CONFIG['epochs']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Training phase\n",
    "        print(\"üîÑ Training...\")\n",
    "        train_loss, train_value_loss, train_weight_loss = train_epoch(\n",
    "            model, train_loader, criterion_value, criterion_weight, optimizer, device\n",
    "        )\n",
    "        \n",
    "        # Validation phase\n",
    "        print(\"üîç Validating...\")\n",
    "        val_loss, val_value_loss, val_weight_loss = evaluate_model(\n",
    "            model, val_loader, criterion_value, criterion_weight, device\n",
    "        )\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Record history\n",
    "        training_history['epoch'].append(epoch + 1)\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['train_value_loss'].append(train_value_loss)\n",
    "        training_history['train_weight_loss'].append(train_weight_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['val_value_loss'].append(val_value_loss)\n",
    "        training_history['val_weight_loss'].append(val_weight_loss)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nüìà Epoch {epoch + 1} Results:\")\n",
    "        print(f\"  Training   - Total: {train_loss:.4f}, Value: {train_value_loss:.4f}, Weight: {train_weight_loss:.4f}\")\n",
    "        print(f\"  Validation - Total: {val_loss:.4f}, Value: {val_value_loss:.4f}, Weight: {val_weight_loss:.4f}\")\n",
    "        print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            print(f\"  ‚úì New best validation loss: {best_val_loss:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'config': TRAINING_CONFIG\n",
    "            }, 'best_model.pth')\n",
    "            print(\"  üíæ Model saved!\")\n",
    "            \n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  ‚è≥ No improvement for {patience_counter} epochs\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"  üõë Early stopping triggered after {patience} epochs without improvement\")\n",
    "                break\n",
    "    \n",
    "    print(\"\\nüéâ Training completed!\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚èπÔ∏è Training interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b748ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation and results\n",
    "print(\"üìä Final Model Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Load best model if available\n",
    "    try:\n",
    "        checkpoint = torch.load('best_model.pth')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"‚úì Loaded best model from checkpoint\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Using current model state (no checkpoint found)\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nüß™ Testing on hold-out test set...\")\n",
    "    test_loss, test_value_loss, test_weight_loss = evaluate_model(\n",
    "        model, test_loader, criterion_value, criterion_weight, device\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüéØ Final Test Results:\")\n",
    "    print(f\"  Total Loss: {test_loss:.4f}\")\n",
    "    print(f\"  Value Loss: {test_value_loss:.4f}\")\n",
    "    print(f\"  Weight Loss: {test_weight_loss:.4f}\")\n",
    "    \n",
    "    # Get some predictions for analysis\n",
    "    model.eval()\n",
    "    sample_predictions = []\n",
    "    sample_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (sequences, commodities, countries, value_targets, weight_targets) in enumerate(test_loader):\n",
    "            if i >= 3:  # Just get a few batches for examples\n",
    "                break\n",
    "                \n",
    "            sequences = sequences.to(device)\n",
    "            commodities = commodities.to(device) \n",
    "            countries = countries.to(device)\n",
    "            \n",
    "            value_outputs, weight_outputs = model(sequences, commodities, countries)\n",
    "            \n",
    "            sample_predictions.extend(list(zip(\n",
    "                value_outputs.cpu().numpy(), \n",
    "                weight_outputs.cpu().numpy()\n",
    "            )))\n",
    "            sample_targets.extend(list(zip(\n",
    "                value_targets.cpu().numpy(), \n",
    "                weight_targets.cpu().numpy()\n",
    "            )))\n",
    "    \n",
    "    # Show some sample predictions\n",
    "    print(f\"\\nüîç Sample Predictions vs Targets:\")\n",
    "    print(\"Value Predictions | Value Targets | Weight Predictions | Weight Targets\")\n",
    "    print(\"-\" * 70)\n",
    "    for i in range(min(10, len(sample_predictions))):\n",
    "        pred_val, pred_weight = sample_predictions[i]\n",
    "        target_val, target_weight = sample_targets[i]\n",
    "        print(f\"{pred_val:>13.2f} | {target_val:>11.2f} | {pred_weight:>16.2f} | {target_weight:>12.2f}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Model evaluation completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97dccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history and save results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üìà Plotting training history...\")\n",
    "\n",
    "try:\n",
    "    # Create subplots for different metrics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Commodity Export Prediction Training Results', fontsize=16)\n",
    "    \n",
    "    # Total Loss\n",
    "    axes[0, 0].plot(training_history['epoch'], training_history['train_loss'], 'b-', label='Training')\n",
    "    axes[0, 0].plot(training_history['epoch'], training_history['val_loss'], 'r-', label='Validation')\n",
    "    axes[0, 0].set_title('Total Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Value Loss\n",
    "    axes[0, 1].plot(training_history['epoch'], training_history['train_value_loss'], 'b-', label='Training')\n",
    "    axes[0, 1].plot(training_history['epoch'], training_history['val_value_loss'], 'r-', label='Validation')\n",
    "    axes[0, 1].set_title('Export Value Loss')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Weight Loss\n",
    "    axes[1, 0].plot(training_history['epoch'], training_history['train_weight_loss'], 'b-', label='Training')\n",
    "    axes[1, 0].plot(training_history['epoch'], training_history['val_weight_loss'], 'r-', label='Validation')\n",
    "    axes[1, 0].set_title('Export Weight Loss')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Prediction vs Target scatter (from samples)\n",
    "    if 'sample_predictions' in locals() and 'sample_targets' in locals():\n",
    "        pred_values = [p[0] for p in sample_predictions[:50]]  # First 50 samples\n",
    "        target_values = [t[0] for t in sample_targets[:50]]\n",
    "        \n",
    "        axes[1, 1].scatter(target_values, pred_values, alpha=0.6)\n",
    "        axes[1, 1].plot([min(target_values), max(target_values)], \n",
    "                       [min(target_values), max(target_values)], 'r--', label='Perfect Prediction')\n",
    "        axes[1, 1].set_title('Value Predictions vs Targets')\n",
    "        axes[1, 1].set_xlabel('Target Values')\n",
    "        axes[1, 1].set_ylabel('Predicted Values')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save training history\n",
    "    import json\n",
    "    with open('training_history.json', 'w') as f:\n",
    "        json.dump(training_history, f, indent=2)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nüìã Training Summary:\")\n",
    "    print(f\"  ‚Ä¢ Total epochs completed: {len(training_history['epoch'])}\")\n",
    "    print(f\"  ‚Ä¢ Best validation loss: {min(training_history['val_loss']):.4f}\")\n",
    "    print(f\"  ‚Ä¢ Final training loss: {training_history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Final validation loss: {training_history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"  ‚Ä¢ Training data size: {DATASET_INFO['train_size']}\")\n",
    "    print(f\"  ‚Ä¢ Test data size: {DATASET_INFO['test_size']}\")\n",
    "    \n",
    "    print(\"\\nüíæ Results saved:\")\n",
    "    print(\"  ‚Ä¢ training_results.png - Training curves visualization\")\n",
    "    print(\"  ‚Ä¢ training_history.json - Detailed training metrics\")\n",
    "    print(\"  ‚Ä¢ best_model.pth - Best model checkpoint\")\n",
    "    \n",
    "    print(\"\\nüéâ Training pipeline completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error plotting results: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a384d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function for new data - THE CALLER!\n",
    "def predict_commodity_exports(model, commodity_name, country_name, historical_data, \n",
    "                            commodity_encoder, country_encoder, device, sequence_length=12):\n",
    "    \"\"\"\n",
    "    Make predictions for commodity exports given historical data\n",
    "    \n",
    "    Args:\n",
    "        model: Trained CommodityS4Model\n",
    "        commodity_name: Name of the commodity\n",
    "        country_name: Name of the destination country\n",
    "        historical_data: List of (month, year, quarter, value, weight) tuples\n",
    "        commodity_encoder: Fitted LabelEncoder for commodities\n",
    "        country_encoder: Fitted LabelEncoder for countries\n",
    "        device: PyTorch device\n",
    "        sequence_length: Length of input sequence\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (predicted_value, predicted_weight)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        # Encode categorical features\n",
    "        commodity_id = commodity_encoder.transform([commodity_name])[0]\n",
    "        country_id = country_encoder.transform([country_name])[0]\n",
    "        \n",
    "        # Prepare sequence data (take last sequence_length points)\n",
    "        sequence_data = historical_data[-sequence_length:]\n",
    "        if len(sequence_data) < sequence_length:\n",
    "            print(f\"Warning: Only {len(sequence_data)} data points available, need {sequence_length}\")\n",
    "            # Pad with zeros or repeat last value\n",
    "            padding_needed = sequence_length - len(sequence_data)\n",
    "            sequence_data = [sequence_data[0]] * padding_needed + sequence_data\n",
    "        \n",
    "        # Convert to tensor\n",
    "        sequence_tensor = torch.tensor(sequence_data, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        commodity_tensor = torch.tensor([commodity_id], dtype=torch.long).to(device)\n",
    "        country_tensor = torch.tensor([country_id], dtype=torch.long).to(device)\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            pred_value, pred_weight = model(sequence_tensor, commodity_tensor, country_tensor)\n",
    "            \n",
    "        return pred_value.item(), pred_weight.item()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error making prediction: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage function\n",
    "def demo_prediction():\n",
    "    \"\"\"Demonstrate how to make predictions with the trained model\"\"\"\n",
    "    print(\"üîÆ Demonstration: Making Predictions\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Get available commodities and countries from the dataset\n",
    "        available_commodities = list(train_dataset.commodity_encoder.classes_)\n",
    "        available_countries = list(train_dataset.country_encoder.classes_)\n",
    "        \n",
    "        print(f\"Available commodities: {available_commodities[:5]}...\")  # Show first 5\n",
    "        print(f\"Available countries: {available_countries[:5]}...\")     # Show first 5\n",
    "        \n",
    "        # Example prediction using the first available commodity and country\n",
    "        if len(available_commodities) > 0 and len(available_countries) > 0:\n",
    "            example_commodity = available_commodities[0]\n",
    "            example_country = available_countries[0]\n",
    "            \n",
    "            # Create dummy historical data (month, year, quarter, normalized_value, normalized_weight)\n",
    "            dummy_historical = [\n",
    "                [6, 2023, 2, 0.1, 0.2],   # June 2023, Q2\n",
    "                [7, 2023, 3, 0.2, 0.3],   # July 2023, Q3\n",
    "                [8, 2023, 3, 0.15, 0.25], # August 2023, Q3\n",
    "                [9, 2023, 3, 0.18, 0.28], # September 2023, Q3\n",
    "                [10, 2023, 4, 0.22, 0.32], # October 2023, Q4\n",
    "                [11, 2023, 4, 0.25, 0.35], # November 2023, Q4\n",
    "                [12, 2023, 4, 0.3, 0.4],   # December 2023, Q4\n",
    "                [1, 2024, 1, 0.28, 0.38],  # January 2024, Q1\n",
    "                [2, 2024, 1, 0.26, 0.36],  # February 2024, Q1\n",
    "                [3, 2024, 1, 0.24, 0.34],  # March 2024, Q1\n",
    "                [4, 2024, 2, 0.27, 0.37],  # April 2024, Q2\n",
    "                [5, 2024, 2, 0.29, 0.39],  # May 2024, Q2\n",
    "            ]\n",
    "            \n",
    "            pred_value, pred_weight = predict_commodity_exports(\n",
    "                model=model,\n",
    "                commodity_name=example_commodity,\n",
    "                country_name=example_country,\n",
    "                historical_data=dummy_historical,\n",
    "                commodity_encoder=train_dataset.commodity_encoder,\n",
    "                country_encoder=train_dataset.country_encoder,\n",
    "                device=device,\n",
    "                sequence_length=TRAINING_CONFIG['sequence_length']\n",
    "            )\n",
    "            \n",
    "            if pred_value is not None:\n",
    "                print(f\"\\nüéØ Prediction Results:\")\n",
    "                print(f\"  Commodity: {example_commodity}\")\n",
    "                print(f\"  Country: {example_country}\")\n",
    "                print(f\"  Predicted Export Value: ${pred_value:,.2f}\")\n",
    "                print(f\"  Predicted Export Weight: {pred_weight:,.2f} kg\")\n",
    "                print(f\"  Value per kg: ${pred_value/pred_weight:.2f}\" if pred_weight > 0 else \"  Value per kg: N/A\")\n",
    "            else:\n",
    "                print(\"‚ùå Prediction failed\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Prediction demonstration completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Demo prediction failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the demonstration\n",
    "demo_prediction()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
